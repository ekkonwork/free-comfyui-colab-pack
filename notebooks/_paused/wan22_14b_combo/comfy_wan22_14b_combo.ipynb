{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_intro"
      },
      "source": [
        "# ComfyUI (GGUF) ? Wan2.2 14B Combo (I2V + First/Last)\n",
        "\n",
        "This notebook includes both Wan2.2 modes in one place:\n",
        "- Image-to-Video (I2V) 14B high/low noise GGUF\n",
        "- First-Frame-to-Last-Frame (camera/control style) 14B high/low noise GGUF\n",
        "\n",
        "Quick start:\n",
        "1. Run all cells top-to-bottom.\n",
        "2. Provide HF token when prompted.\n",
        "3. In ComfyUI, load the corresponding workflow JSON for the mode you want.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_before_cell_install"
      },
      "source": [
        "## 1) Installation\n",
        "Install ComfyUI, ComfyUI-Manager, ComfyUI-GGUF, and swap for better Colab stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_install"
      },
      "outputs": [],
      "source": [
        "# @title 1) Install ComfyUI + Manager + ComfyUI-GGUF (with swap)\n",
        "import os\n",
        "\n",
        "# Small protection against RAM-related crashes\n",
        "if not os.path.exists('/swapfile'):\n",
        "    print('Creating swap (8GB)...')\n",
        "    !sudo fallocate -l 8G /swapfile\n",
        "    !sudo chmod 600 /swapfile\n",
        "    !sudo mkswap /swapfile\n",
        "    !sudo swapon /swapfile\n",
        "    print('Swap enabled.')\n",
        "\n",
        "# Useful packages\n",
        "!apt-get -y update -qq\n",
        "!apt-get -y install -qq aria2 ffmpeg\n",
        "\n",
        "# More predictable CUDA allocator (sometimes helps reduce fragmentation)\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "\n",
        "# ComfyUI\n",
        "if not os.path.exists('/content/ComfyUI'):\n",
        "    print('Cloning ComfyUI...')\n",
        "    !git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "print('Installing python requirements...')\n",
        "!pip install -U pip\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Attention acceleration pack (T4-safe)\n",
        "ENABLE_ACCEL_PACK = True  # Set False to skip optional attention accelerators.\n",
        "\n",
        "if ENABLE_ACCEL_PACK:\n",
        "    import sys\n",
        "    import subprocess\n",
        "\n",
        "    def pip_try(spec):\n",
        "        print(f\"Installing optional accelerator: {spec}\")\n",
        "        return subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', spec], check=False).returncode == 0\n",
        "\n",
        "    import torch\n",
        "    gpu_name = 'CPU'\n",
        "    cc_major, cc_minor = (0, 0)\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
        "\n",
        "    print(f\"GPU detected: {gpu_name} (sm{cc_major}{cc_minor})\")\n",
        "\n",
        "    x_ok = pip_try('xformers>=0.0.28.post3')\n",
        "    print('xformers:', 'OK' if x_ok else 'FAILED (continuing)')\n",
        "\n",
        "    # FlashAttention-3 is Hopper-only; FlashAttention-2 and SageAttention target Ampere+.\n",
        "    if cc_major >= 8:\n",
        "        fa_ok = pip_try('flash-attn>=2.7.0.post2')\n",
        "        sage_ok = pip_try('sageattention>=2.1.1')\n",
        "        print('flash-attn:', 'OK' if fa_ok else 'FAILED (continuing)')\n",
        "        print('sageattention:', 'OK' if sage_ok else 'FAILED (continuing)')\n",
        "    else:\n",
        "        print('Skipping flash-attn and sageattention on this GPU: T4/Turing (sm75) uses xformers path.')\n",
        "else:\n",
        "    print('Attention acceleration pack disabled by user (ENABLE_ACCEL_PACK=False).')\n",
        "\n",
        "\n",
        "# ComfyUI-Manager\n",
        "if not os.path.exists('custom_nodes/ComfyUI-Manager'):\n",
        "    !git clone https://github.com/ltdrdata/ComfyUI-Manager.git custom_nodes/ComfyUI-Manager\n",
        "\n",
        "# ComfyUI-GGUF\n",
        "if not os.path.exists('custom_nodes/ComfyUI-GGUF'):\n",
        "    print('Installing ComfyUI-GGUF...')\n",
        "    !git clone https://github.com/city96/ComfyUI-GGUF.git custom_nodes/ComfyUI-GGUF\n",
        "    !pip install -r custom_nodes/ComfyUI-GGUF/requirements.txt\n",
        "\n",
        "# ComfyUI-KJNodes\n",
        "if not os.path.exists('custom_nodes/comfyui-kjnodes'):\n",
        "    print('Installing ComfyUI-KJNodes...')\n",
        "    !git clone https://github.com/kijai/ComfyUI-KJNodes.git custom_nodes/comfyui-kjnodes\n",
        "if os.path.exists('custom_nodes/comfyui-kjnodes/requirements.txt'):\n",
        "    !pip install -r custom_nodes/comfyui-kjnodes/requirements.txt\n",
        "\n",
        "\n",
        "\n",
        "# Hugging Face authentication (interactive prompt)\n",
        "!pip install -q huggingface_hub\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = getpass('Enter Hugging Face token: ').strip()\n",
        "if not hf_token:\n",
        "    raise RuntimeError('Hugging Face token is required for this notebook run.')\n",
        "\n",
        "login(token=hf_token, add_to_git_credential=False)\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "print('HF auth: OK')\n",
        "\n",
        "# Civitai authentication (for LoRA downloads)\n",
        "civitai_token = getpass('Enter Civitai API token (optional, press Enter to skip): ').strip()\n",
        "if civitai_token:\n",
        "    os.environ['CIVITAI_API_TOKEN'] = civitai_token\n",
        "    print('Civitai auth: OK')\n",
        "else:\n",
        "    print('Civitai token not set. Civitai downloads may fail (403).')\n",
        "\n",
        "print('Done.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_before_cell_config"
      },
      "source": [
        "## 2) Quant Settings\n",
        "Choose GGUF quant defaults and review rough file-size/VRAM estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_config"
      },
      "outputs": [],
      "source": [
        "# @title 2) Settings: GGUF quant selection + size/VRAM table\n",
        "import math\n",
        "\n",
        "# ---- Auto quant by VRAM ----\n",
        "AUTO_QUANT_BY_VRAM = True\n",
        "AUTO_QUANT_VRAM_FRACTION = 0.9\n",
        "AUTO_VRAM_FALLBACK_GB = 14.0\n",
        "\n",
        "\n",
        "# ---- Mode toggles ----\n",
        "DOWNLOAD_I2V = True\n",
        "DOWNLOAD_FLF2V = False\n",
        "\n",
        "# Wan2.2 I2V 14B GGUF (both are required)\n",
        "I2V_HIGH_QUANT = 'Q2_K'  # options: Q2_K, Q3_K_S, Q3_K_M, Q3_K_L, Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K, Q8_0\n",
        "I2V_LOW_QUANT  = 'Q2_K'  # options: Q2_K, Q3_K_S, Q3_K_M, Q3_K_L, Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K, Q8_0\n",
        "\n",
        "# Wan2.2 First/Last (Fun Camera A14B GGUF, both are required)\n",
        "FLF2V_HIGH_QUANT = 'Q2_K'  # options: Q2_K, Q3_K_S, Q3_K_M, Q4_0, Q4_1, Q4_K_S, Q4_K_M, Q5_0, Q5_1, Q5_K_S, Q5_K_M, Q6_K, Q8_0\n",
        "FLF2V_LOW_QUANT  = 'Q2_K'  # options: Q2_K, Q3_K_S, Q3_K_M, Q4_0, Q4_1, Q4_K_S, Q4_K_M, Q5_0, Q5_1, Q5_K_S, Q5_K_M, Q6_K, Q8_0\n",
        "\n",
        "# Text encoder GGUF for Wan family\n",
        "UMT5_QUANT = 'Q3_K_S'  # options: Q3_K_S, Q3_K_M, Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K, Q8_0, F16, F32\n",
        "\n",
        "# VRAM helper flags\n",
        "OFFLOAD_TEXT_ENCODER = True\n",
        "DOWNLOAD_I2V_LIGHTNING_LORA = False\n",
        "\n",
        "# ---- File-size tables (GB), checked from repo headers ----\n",
        "WAN_I2V_QUANTS_GB = {\n",
        "    'Q2_K': 5.30,\n",
        "    'Q3_K_S': 6.52,\n",
        "    'Q3_K_M': 7.18,\n",
        "    'Q3_K_L': 7.79,\n",
        "    'Q4_K_S': 8.75,\n",
        "    'Q4_K_M': 9.65,\n",
        "    'Q5_K_S': 10.14,\n",
        "    'Q5_K_M': 10.79,\n",
        "    'Q6_K': 12.00,\n",
        "    'Q8_0': 15.41,\n",
        "}\n",
        "\n",
        "WAN_FLF2V_QUANTS_GB = {\n",
        "    'Q2_K': 6.32,\n",
        "    'Q3_K_S': 7.53,\n",
        "    'Q3_K_M': 8.19,\n",
        "    'Q4_0': 9.57,\n",
        "    'Q4_1': 10.28,\n",
        "    'Q4_K_S': 9.76,\n",
        "    'Q4_K_M': 10.67,\n",
        "    'Q5_0': 11.33,\n",
        "    'Q5_1': 12.03,\n",
        "    'Q5_K_S': 11.15,\n",
        "    'Q5_K_M': 11.81,\n",
        "    'Q6_K': 13.02,\n",
        "    'Q8_0': 16.42,\n",
        "}\n",
        "\n",
        "UMT5_GGUF_GB = {\n",
        "    'Q3_K_S': 2.86,\n",
        "    'Q3_K_M': 3.06,\n",
        "    'Q4_K_S': 3.50,\n",
        "    'Q4_K_M': 3.66,\n",
        "    'Q5_K_S': 4.05,\n",
        "    'Q5_K_M': 4.15,\n",
        "    'Q6_K': 4.67,\n",
        "    'Q8_0': 6.04,\n",
        "    'F16': 11.37,\n",
        "    'F32': 22.73,\n",
        "}\n",
        "\n",
        "WAN22_VAE_GB = 1.41\n",
        "WAN21_VAE_GB = 0.25\n",
        "CLIP_VISION_H_GB = 1.26\n",
        "I2V_LORA_EACH_GB = 1.23\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def detect_total_vram_gb(default=AUTO_VRAM_FALLBACK_GB):\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    except Exception as e:\n",
        "        print(f\"VRAM detection failed ({e}); using fallback {default:.2f} GB\")\n",
        "    return default\n",
        "\n",
        "\n",
        "def pick_best_quant(size_dict, budget_gb, overhead=1.10):\n",
        "    items = sorted(size_dict.items(), key=lambda kv: kv[1])\n",
        "    fitting = [(k, v) for k, v in items if (v * overhead) <= budget_gb]\n",
        "    if fitting:\n",
        "        return fitting[-1][0]\n",
        "    return items[0][0]\n",
        "\n",
        "\n",
        "def est_vram_gb(size_gb, overhead=1.10):\n",
        "    return size_gb * overhead\n",
        "\n",
        "\n",
        "def print_table(title, d):\n",
        "    print('\\n' + title)\n",
        "    print('-' * len(title))\n",
        "    for k, v in d.items():\n",
        "        print(f\"{k:8s}  file~{v:5.2f} GB   VRAM~{est_vram_gb(v):5.2f} GB\")\n",
        "\n",
        "\n",
        "# ---- Auto apply quant choice based on detected VRAM ----\n",
        "TOTAL_VRAM_GB = detect_total_vram_gb()\n",
        "AUTO_BUDGET_GB = TOTAL_VRAM_GB * AUTO_QUANT_VRAM_FRACTION\n",
        "print(f\"\\nGPU VRAM detected: ~{TOTAL_VRAM_GB:.2f} GB | Auto budget (75%): ~{AUTO_BUDGET_GB:.2f} GB\")\n",
        "\n",
        "if AUTO_QUANT_BY_VRAM:\n",
        "    i2v_candidates = list(WAN_I2V_QUANTS_GB.keys()) if DOWNLOAD_I2V else [I2V_HIGH_QUANT]\n",
        "    flf_candidates = list(WAN_FLF2V_QUANTS_GB.keys()) if DOWNLOAD_FLF2V else [FLF2V_HIGH_QUANT]\n",
        "    umt5_candidates = list(UMT5_GGUF_GB.keys()) if not OFFLOAD_TEXT_ENCODER else [UMT5_QUANT]\n",
        "\n",
        "    best_fit = None\n",
        "    best_any = None\n",
        "\n",
        "    for i_q in i2v_candidates:\n",
        "        for f_q in flf_candidates:\n",
        "            for t_q in umt5_candidates:\n",
        "                text_side = 0.0 if OFFLOAD_TEXT_ENCODER else est_vram_gb(UMT5_GGUF_GB[t_q])\n",
        "\n",
        "                i2v_total = 0.0\n",
        "                if DOWNLOAD_I2V:\n",
        "                    i2v_total = 2 * est_vram_gb(WAN_I2V_QUANTS_GB[i_q]) + est_vram_gb(WAN22_VAE_GB) + text_side\n",
        "                    if DOWNLOAD_I2V_LIGHTNING_LORA:\n",
        "                        i2v_total += est_vram_gb(I2V_LORA_EACH_GB * 2)\n",
        "\n",
        "                flf_total = 0.0\n",
        "                if DOWNLOAD_FLF2V:\n",
        "                    flf_total = (\n",
        "                        2 * est_vram_gb(WAN_FLF2V_QUANTS_GB[f_q])\n",
        "                        + est_vram_gb(WAN21_VAE_GB)\n",
        "                        + est_vram_gb(CLIP_VISION_H_GB)\n",
        "                        + text_side\n",
        "                    )\n",
        "\n",
        "                peak_total = max(i2v_total, flf_total)\n",
        "                if not DOWNLOAD_I2V and not DOWNLOAD_FLF2V:\n",
        "                    peak_total = text_side\n",
        "\n",
        "                score = (\n",
        "                    WAN_I2V_QUANTS_GB[i_q] if DOWNLOAD_I2V else 0.0,\n",
        "                    WAN_FLF2V_QUANTS_GB[f_q] if DOWNLOAD_FLF2V else 0.0,\n",
        "                    UMT5_GGUF_GB[t_q] if not OFFLOAD_TEXT_ENCODER else 0.0,\n",
        "                )\n",
        "\n",
        "                if (best_any is None) or (peak_total < best_any[0]):\n",
        "                    best_any = (peak_total, i_q, f_q, t_q)\n",
        "\n",
        "                if peak_total <= AUTO_BUDGET_GB:\n",
        "                    if (best_fit is None) or (score > best_fit[0]) or (score == best_fit[0] and peak_total < best_fit[1]):\n",
        "                        best_fit = (score, peak_total, i_q, f_q, t_q)\n",
        "\n",
        "    chosen = best_fit if best_fit is not None else best_any\n",
        "    I2V_HIGH_QUANT = I2V_LOW_QUANT = chosen[2]\n",
        "    FLF2V_HIGH_QUANT = FLF2V_LOW_QUANT = chosen[3]\n",
        "    if not OFFLOAD_TEXT_ENCODER:\n",
        "        UMT5_QUANT = chosen[4]\n",
        "\n",
        "    print(f\"Auto quant active: I2V={I2V_HIGH_QUANT}, FLF2V={FLF2V_HIGH_QUANT}, UMT5={UMT5_QUANT}\")\n",
        "\n",
        "print_table('Wan2.2 I2V 14B GGUF quants (high/low)', WAN_I2V_QUANTS_GB)\n",
        "print_table('Wan2.2 First/Last (Fun Camera) GGUF quants (high/low)', WAN_FLF2V_QUANTS_GB)\n",
        "print_table('UMT5 encoder GGUF quants', UMT5_GGUF_GB)\n",
        "\n",
        "print('\\nSelected:')\n",
        "print('  DOWNLOAD_I2V              =', DOWNLOAD_I2V)\n",
        "print('  DOWNLOAD_FLF2V            =', DOWNLOAD_FLF2V)\n",
        "print('  I2V_HIGH_QUANT / LOW      =', I2V_HIGH_QUANT, '/', I2V_LOW_QUANT)\n",
        "print('  FLF2V_HIGH_QUANT / LOW    =', FLF2V_HIGH_QUANT, '/', FLF2V_LOW_QUANT)\n",
        "print('  UMT5_QUANT                =', UMT5_QUANT)\n",
        "print('  OFFLOAD_TEXT_ENCODER      =', OFFLOAD_TEXT_ENCODER)\n",
        "print('  DOWNLOAD_I2V_LIGHTNING_LORA =', DOWNLOAD_I2V_LIGHTNING_LORA)\n",
        "\n",
        "# Rough GPU totals per mode\n",
        "text_side = est_vram_gb(UMT5_GGUF_GB[UMT5_QUANT])\n",
        "if OFFLOAD_TEXT_ENCODER:\n",
        "    text_side = 0.0\n",
        "\n",
        "i2v_total = 0.0\n",
        "if DOWNLOAD_I2V:\n",
        "    i2v_total += est_vram_gb(WAN_I2V_QUANTS_GB[I2V_HIGH_QUANT])\n",
        "    i2v_total += est_vram_gb(WAN_I2V_QUANTS_GB[I2V_LOW_QUANT])\n",
        "    i2v_total += est_vram_gb(WAN22_VAE_GB)\n",
        "    i2v_total += text_side\n",
        "    if DOWNLOAD_I2V_LIGHTNING_LORA:\n",
        "        i2v_total += est_vram_gb(I2V_LORA_EACH_GB * 2)\n",
        "\n",
        "flf_total = 0.0\n",
        "if DOWNLOAD_FLF2V:\n",
        "    flf_total += est_vram_gb(WAN_FLF2V_QUANTS_GB[FLF2V_HIGH_QUANT])\n",
        "    flf_total += est_vram_gb(WAN_FLF2V_QUANTS_GB[FLF2V_LOW_QUANT])\n",
        "    flf_total += est_vram_gb(WAN21_VAE_GB)\n",
        "    flf_total += est_vram_gb(CLIP_VISION_H_GB)\n",
        "    flf_total += text_side\n",
        "\n",
        "print(f\"\\nRough GPU VRAM (I2V mode):   ~{i2v_total:.2f} GB\")\n",
        "print(f\"Rough GPU VRAM (FLF2V mode): ~{flf_total:.2f} GB\")\n",
        "print('Note: these are rough weight-only estimates; frame count/resolution can increase peak usage a lot.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_before_cell_models"
      },
      "source": [
        "## 3) Model Download\n",
        "Download the required model files (and optional components) into ComfyUI folders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_models"
      },
      "outputs": [],
      "source": [
        "# @title 3) Download models (Wan2.2 I2V + First/Last in one notebook)\n",
        "import os\n",
        "\n",
        "COMFY = '/content/ComfyUI'\n",
        "\n",
        "UNET_DIR = f'{COMFY}/models/unet'\n",
        "DIFF_DIR = f'{COMFY}/models/diffusion_models'\n",
        "TE_DIR   = f'{COMFY}/models/text_encoders'\n",
        "CLIP_DIR = f'{COMFY}/models/clip'\n",
        "VAE_DIR  = f'{COMFY}/models/vae'\n",
        "CLIPV_DIR = f'{COMFY}/models/clip_vision'\n",
        "LORA_DIR = f'{COMFY}/models/loras'\n",
        "WORKFLOW_DIR = f'{COMFY}/user/default/workflows'\n",
        "\n",
        "for d in [UNET_DIR, DIFF_DIR, TE_DIR, CLIP_DIR, VAE_DIR, CLIPV_DIR, LORA_DIR, WORKFLOW_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "def dl(url, outdir, fname):\n",
        "    outpath = os.path.join(outdir, fname)\n",
        "    if os.path.exists(outpath):\n",
        "        print('Already exists:', outpath)\n",
        "        return\n",
        "    print('Downloading:', fname)\n",
        "\n",
        "    hf_token = os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HF_TOKEN')\n",
        "    if hf_token and 'huggingface.co' in url:\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --header=\"Authorization: Bearer {hf_token}\" \"{url}\" -d \"{outdir}\" -o \"{fname}\"\n",
        "    else:\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{outdir}\" -o \"{fname}\"\n",
        "\n",
        "\n",
        "def link_if_missing(src, dst):\n",
        "    if not os.path.exists(dst):\n",
        "        !ln -s \"{src}\" \"{dst}\"\n",
        "\n",
        "\n",
        "# Common text encoder (GGUF)\n",
        "umt5_fname = f\"umt5-xxl-encoder-{UMT5_QUANT}.gguf\"\n",
        "umt5_url = f\"https://huggingface.co/city96/umt5-xxl-encoder-gguf/resolve/main/{umt5_fname}\"\n",
        "dl(umt5_url, TE_DIR, umt5_fname)\n",
        "link_if_missing(f\"{TE_DIR}/{umt5_fname}\", f\"{CLIP_DIR}/{umt5_fname}\")\n",
        "\n",
        "# Official VAE files\n",
        "wan22_vae_fname = 'wan2.2_vae.safetensors'\n",
        "wan22_vae_url = 'https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan2.2_vae.safetensors'\n",
        "dl(wan22_vae_url, VAE_DIR, wan22_vae_fname)\n",
        "\n",
        "wan21_vae_fname = 'wan_2.1_vae.safetensors'\n",
        "wan21_vae_url = 'https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors'\n",
        "dl(wan21_vae_url, VAE_DIR, wan21_vae_fname)\n",
        "\n",
        "clipv_fname = 'clip_vision_h.safetensors'\n",
        "clipv_url = 'https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors'\n",
        "dl(clipv_url, CLIPV_DIR, clipv_fname)\n",
        "\n",
        "# I2V 14B GGUF (high+low)\n",
        "i2v_high_fname = f\"wan2.2_i2v_high_noise_14B_{I2V_HIGH_QUANT}.gguf\"\n",
        "i2v_low_fname  = f\"wan2.2_i2v_low_noise_14B_{I2V_LOW_QUANT}.gguf\"\n",
        "\n",
        "if DOWNLOAD_I2V:\n",
        "    i2v_high_url = f\"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/{i2v_high_fname}\"\n",
        "    i2v_low_url  = f\"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/{i2v_low_fname}\"\n",
        "    dl(i2v_high_url, UNET_DIR, i2v_high_fname)\n",
        "    dl(i2v_low_url, UNET_DIR, i2v_low_fname)\n",
        "    link_if_missing(f\"{UNET_DIR}/{i2v_high_fname}\", f\"{DIFF_DIR}/{i2v_high_fname}\")\n",
        "    link_if_missing(f\"{UNET_DIR}/{i2v_low_fname}\", f\"{DIFF_DIR}/{i2v_low_fname}\")\n",
        "\n",
        "# First/Last (Fun Camera A14B GGUF, high+low)\n",
        "flf_high_fname = f\"Wan2.2-Fun-A14B-Control-Camera-HighNoise-{FLF2V_HIGH_QUANT}.gguf\"\n",
        "flf_low_fname  = f\"Wan2.2-Fun-A14B-Control-Camera-LowNoise-{FLF2V_LOW_QUANT}.gguf\"\n",
        "\n",
        "if DOWNLOAD_FLF2V:\n",
        "    flf_high_url = f\"https://huggingface.co/QuantStack/Wan2.2-Fun-A14B-Control-Camera-GGUF/resolve/main/HighNoise/{flf_high_fname}\"\n",
        "    flf_low_url  = f\"https://huggingface.co/QuantStack/Wan2.2-Fun-A14B-Control-Camera-GGUF/resolve/main/LowNoise/{flf_low_fname}\"\n",
        "    dl(flf_high_url, UNET_DIR, flf_high_fname)\n",
        "    dl(flf_low_url, UNET_DIR, flf_low_fname)\n",
        "    link_if_missing(f\"{UNET_DIR}/{flf_high_fname}\", f\"{DIFF_DIR}/{flf_high_fname}\")\n",
        "    link_if_missing(f\"{UNET_DIR}/{flf_low_fname}\", f\"{DIFF_DIR}/{flf_low_fname}\")\n",
        "\n",
        "# Optional I2V 4-step LoRAs (official Comfy-Org repackaged)\n",
        "if DOWNLOAD_I2V and DOWNLOAD_I2V_LIGHTNING_LORA:\n",
        "    lora_high = 'wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors'\n",
        "    lora_low  = 'wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors'\n",
        "    lora_high_url = f\"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/{lora_high}\"\n",
        "    lora_low_url  = f\"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/{lora_low}\"\n",
        "    dl(lora_high_url, LORA_DIR, lora_high)\n",
        "    dl(lora_low_url, LORA_DIR, lora_low)\n",
        "\n",
        "# Official workflows\n",
        "if DOWNLOAD_I2V:\n",
        "    dl('https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/master/wan22/image_to_video_wan22_14B.json', WORKFLOW_DIR, 'image_to_video_wan22_14B.json')\n",
        "if DOWNLOAD_FLF2V:\n",
        "    dl('https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/master/wan/camera_image_to_video_wan_example.json', WORKFLOW_DIR, 'camera_image_to_video_wan_example.json')\n",
        "\n",
        "print('Done downloading models.')\n",
        "print('UMT5 GGUF:', umt5_fname)\n",
        "print('VAE files:', wan22_vae_fname, 'and', wan21_vae_fname)\n",
        "print('ClipVision:', clipv_fname)\n",
        "if DOWNLOAD_I2V:\n",
        "    print('I2V models:', i2v_high_fname, '|', i2v_low_fname)\n",
        "if DOWNLOAD_FLF2V:\n",
        "    print('First/Last models:', flf_high_fname, '|', flf_low_fname)\n",
        "\n",
        "\n",
        "# ---- Lenovo UltraReal LoRA pack (Civitai) ----\n",
        "DOWNLOAD_LENOVO_ULTRAREAL_LORAS = True\n",
        "CIVITAI_API_TOKEN = os.environ.get('CIVITAI_API_TOKEN', '').strip()  # optional, helps if Civitai returns 403.\n",
        "CURRENT_BASE_MODELS = ['Wan Video 14B t2v']\n",
        "\n",
        "if DOWNLOAD_LENOVO_ULTRAREAL_LORAS:\n",
        "    import json\n",
        "    import subprocess\n",
        "    import urllib.request\n",
        "    import urllib.parse\n",
        "\n",
        "    LORA_DIR = f'{COMFY}/models/loras'\n",
        "    os.makedirs(LORA_DIR, exist_ok=True)\n",
        "\n",
        "    def add_civitai_token(url, token):\n",
        "        if not token:\n",
        "            return url\n",
        "        parts = urllib.parse.urlsplit(url)\n",
        "        query = urllib.parse.parse_qsl(parts.query, keep_blank_values=True)\n",
        "        if not any(k == 'token' for k, _ in query):\n",
        "            query.append(('token', token))\n",
        "        return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, urllib.parse.urlencode(query), parts.fragment))\n",
        "\n",
        "    def dl_civitai(url, outdir, fname, token=''):\n",
        "        outpath = os.path.join(outdir, fname)\n",
        "        if os.path.exists(outpath):\n",
        "            print('Already exists:', outpath)\n",
        "            return True\n",
        "\n",
        "        final_url = add_civitai_token(url, token)\n",
        "        print('Downloading LoRA:', fname)\n",
        "        cmd = ['aria2c', '--console-log-level=error', '-c', '-x', '8', '-s', '8', '-k', '1M', final_url, '-d', outdir, '-o', fname]\n",
        "        rc = subprocess.run(cmd, check=False).returncode\n",
        "        if rc != 0 and not token:\n",
        "            print('  Download failed without token. Set CIVITAI_API_TOKEN env var and retry Cell 3.')\n",
        "        return rc == 0\n",
        "\n",
        "    try:\n",
        "        req = urllib.request.Request(\n",
        "            'https://civitai.com/api/v1/models/1662740',\n",
        "            headers={'User-Agent': 'Mozilla/5.0'}\n",
        "        )\n",
        "        with urllib.request.urlopen(req, timeout=30) as resp:\n",
        "            civitai_model = json.loads(resp.read().decode('utf-8'))\n",
        "\n",
        "        versions = civitai_model.get('modelVersions', [])\n",
        "        print('\\nLenovo UltraReal LoRA versions found:', len(versions))\n",
        "        if not CURRENT_BASE_MODELS:\n",
        "            print('Current notebook base-model tags: (none matched explicitly)')\n",
        "        else:\n",
        "            print('Current notebook base-model tags:', ', '.join(CURRENT_BASE_MODELS))\n",
        "\n",
        "        for mv in versions:\n",
        "            base = mv.get('baseModel', 'Unknown')\n",
        "            match = base in CURRENT_BASE_MODELS\n",
        "            mark = 'MATCH' if match else 'OTHER'\n",
        "            files = [f for f in mv.get('files', []) if f.get('type') == 'Model']\n",
        "            print(f\"- [{mark}] {base} :: {mv.get('name', 'Unnamed version')} ({len(files)} file(s))\")\n",
        "\n",
        "        ok_count = 0\n",
        "        total_count = 0\n",
        "        matched_versions = [mv for mv in versions if mv.get('baseModel', 'Unknown') in CURRENT_BASE_MODELS]\n",
        "\n",
        "        if not matched_versions:\n",
        "            print('No MATCH versions found for this notebook base-model tag set. Skipping LoRA download.')\n",
        "\n",
        "        for mv in matched_versions:\n",
        "            for fobj in mv.get('files', []):\n",
        "                if fobj.get('type') != 'Model':\n",
        "                    continue\n",
        "                total_count += 1\n",
        "                raw_name = (fobj.get('name') or '').strip()\n",
        "                base_tag = ''.join(ch if ch.isalnum() else '_' for ch in str(mv.get('baseModel', 'unknown'))).strip('_').lower() or 'unknown'\n",
        "                fname = f\"lenovo_ultrareal_{base_tag}_v{mv.get('id', 'x')}_f{fobj.get('id', 'x')}.safetensors\"\n",
        "                if raw_name and raw_name != fname:\n",
        "                    print(f\"  Civitai file name: {raw_name} -> saved as {fname}\")\n",
        "                if dl_civitai(fobj.get('downloadUrl', ''), LORA_DIR, fname, CIVITAI_API_TOKEN):\n",
        "                    ok_count += 1\n",
        "\n",
        "        print(f\"\\nLenovo UltraReal LoRA download summary (MATCH only): {ok_count}/{total_count} files ready in {LORA_DIR}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Failed to fetch Lenovo UltraReal LoRA metadata from Civitai:', e)\n",
        "        print('Tip: this may require CIVITAI_API_TOKEN or temporary Civitai availability.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_before_cell_check"
      },
      "source": [
        "## 4) Download Check\n",
        "Verify downloaded file sizes and calculate a rough VRAM estimate from real files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_check"
      },
      "outputs": [],
      "source": [
        "# @title 4) Verify downloads: actual file sizes and VRAM estimate\n",
        "import os\n",
        "\n",
        "\n",
        "def size_gb(path):\n",
        "    return os.path.getsize(path) / (1024**3)\n",
        "\n",
        "\n",
        "def est_vram_from_file(path, overhead=1.10):\n",
        "    return size_gb(path) * overhead\n",
        "\n",
        "\n",
        "paths = [\n",
        "    ('/content/ComfyUI/models/text_encoders', f\"umt5-xxl-encoder-{UMT5_QUANT}.gguf\"),\n",
        "    ('/content/ComfyUI/models/vae', 'wan2.2_vae.safetensors'),\n",
        "    ('/content/ComfyUI/models/vae', 'wan_2.1_vae.safetensors'),\n",
        "    ('/content/ComfyUI/models/clip_vision', 'clip_vision_h.safetensors'),\n",
        "]\n",
        "\n",
        "if DOWNLOAD_I2V:\n",
        "    paths.append(('/content/ComfyUI/models/unet', f\"wan2.2_i2v_high_noise_14B_{I2V_HIGH_QUANT}.gguf\"))\n",
        "    paths.append(('/content/ComfyUI/models/unet', f\"wan2.2_i2v_low_noise_14B_{I2V_LOW_QUANT}.gguf\"))\n",
        "\n",
        "if DOWNLOAD_FLF2V:\n",
        "    paths.append(('/content/ComfyUI/models/unet', f\"Wan2.2-Fun-A14B-Control-Camera-HighNoise-{FLF2V_HIGH_QUANT}.gguf\"))\n",
        "    paths.append(('/content/ComfyUI/models/unet', f\"Wan2.2-Fun-A14B-Control-Camera-LowNoise-{FLF2V_LOW_QUANT}.gguf\"))\n",
        "\n",
        "if DOWNLOAD_I2V and DOWNLOAD_I2V_LIGHTNING_LORA:\n",
        "    paths.append(('/content/ComfyUI/models/loras', 'wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors'))\n",
        "    paths.append(('/content/ComfyUI/models/loras', 'wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors'))\n",
        "\n",
        "print('Files:')\n",
        "total_vram = 0.0\n",
        "for d, f in paths:\n",
        "    p = os.path.join(d, f)\n",
        "    if not os.path.exists(p):\n",
        "        print('MISSING:', p)\n",
        "        continue\n",
        "    s = size_gb(p)\n",
        "    v = est_vram_from_file(p)\n",
        "    total_vram += v\n",
        "    print(f\"- {p}\\n  size~{s:.2f} GB  -> VRAM(weights)~{v:.2f} GB\")\n",
        "\n",
        "print(f\"\\nSum VRAM(weights) if all kept on GPU at once: ~{total_vram:.2f} GB\")\n",
        "print('Reminder: in real workflows, active VRAM depends on mode, frame count, resolution and offloading.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_before_cell_run"
      },
      "source": [
        "## 5) Launch ComfyUI\n",
        "Start ComfyUI and use the Cloudflare Tunnel URL shown in the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_run"
      },
      "outputs": [],
      "source": [
        "# @title 5) Launch ComfyUI (Cloudflare Tunnel) - stable mode\n",
        "import subprocess, threading, time, socket, os, re, sys, urllib.request\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "# Install cloudflared (if not installed yet)\n",
        "if not os.path.exists(\"cloudflared-linux-amd64.deb\"):\n",
        "    !wget -q -nc https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "    !dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "\n",
        "def wait_port(host=\"127.0.0.1\", port=8188, timeout=240):\n",
        "    t0 = time.time()\n",
        "    while time.time() - t0 < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((host, port), timeout=1):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(0.5)\n",
        "    return False\n",
        "\n",
        "\n",
        "def verify_tunnel_url(url, timeout=10):\n",
        "    base = url.rstrip('/')\n",
        "    candidates = [\n",
        "        base + '/',\n",
        "        base + '/api/system_stats',\n",
        "        base + '/object_info',\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        try:\n",
        "            req = urllib.request.Request(c, headers={'User-Agent': 'Mozilla/5.0'}, method='GET')\n",
        "            with urllib.request.urlopen(req, timeout=timeout) as r:\n",
        "                code = r.getcode()\n",
        "                if 200 <= code < 500:\n",
        "                    return True\n",
        "        except Exception:\n",
        "            pass\n",
        "    return False\n",
        "\n",
        "\n",
        "def wait_reachable_stable(url, proc, warmup_timeout=180, stable_successes=3, probe_interval=2.5):\n",
        "    t0 = time.time()\n",
        "    ok_streak = 0\n",
        "    while time.time() - t0 < warmup_timeout:\n",
        "        if proc.poll() is not None:\n",
        "            return False\n",
        "        if verify_tunnel_url(url, timeout=10):\n",
        "            ok_streak += 1\n",
        "            if ok_streak >= stable_successes:\n",
        "                return True\n",
        "        else:\n",
        "            ok_streak = 0\n",
        "        time.sleep(probe_interval)\n",
        "    return False\n",
        "\n",
        "\n",
        "def start_cloudflare_tunnel_once(port=8188, protocol='http2', read_timeout=150, warmup_timeout=180):\n",
        "    print(f\"\\nStarting Cloudflare Quick Tunnel (protocol={protocol})...\\n\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    cmd = [\n",
        "        'cloudflared', 'tunnel',\n",
        "        '--no-autoupdate',\n",
        "        '--url', f'http://127.0.0.1:{port}',\n",
        "        '--protocol', protocol,\n",
        "        '--loglevel', 'info',\n",
        "    ]\n",
        "\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "    )\n",
        "\n",
        "    tunnel_patterns = [\n",
        "        re.compile(r'https://[a-z0-9-]+\\.trycloudflare\\.com', re.I),\n",
        "        re.compile(r'https://[a-z0-9-]+\\.cfargotunnel\\.com', re.I),\n",
        "    ]\n",
        "    ignore_patterns = [\n",
        "        re.compile(r'https://www\\.cloudflare\\.com/website-terms/?', re.I),\n",
        "    ]\n",
        "\n",
        "    t0 = time.time()\n",
        "    url = None\n",
        "\n",
        "    while time.time() - t0 < read_timeout:\n",
        "        line = proc.stdout.readline()\n",
        "        if not line:\n",
        "            if proc.poll() is not None:\n",
        "                break\n",
        "            time.sleep(0.1)\n",
        "            continue\n",
        "\n",
        "        s = line.strip()\n",
        "\n",
        "        if any(ip.search(s) for ip in ignore_patterns):\n",
        "            continue\n",
        "\n",
        "        for pat in tunnel_patterns:\n",
        "            m = pat.search(s)\n",
        "            if m:\n",
        "                url = m.group(0)\n",
        "                break\n",
        "        if url:\n",
        "            break\n",
        "\n",
        "    if not url:\n",
        "        print('Failed to parse tunnel URL.')\n",
        "        if proc.poll() is None:\n",
        "            proc.terminate()\n",
        "            try:\n",
        "                proc.wait(timeout=5)\n",
        "            except Exception:\n",
        "                proc.kill()\n",
        "        return None, None\n",
        "\n",
        "    print(f'Found tunnel URL: {url}')\n",
        "    print('Waiting for Cloudflare propagation and stable readiness...')\n",
        "\n",
        "    if wait_reachable_stable(url, proc, warmup_timeout=warmup_timeout, stable_successes=3, probe_interval=2.5):\n",
        "        print('\\n--------------------------------------------------')\n",
        "        print('YOUR LINK:', url)\n",
        "        print('--------------------------------------------------\\n')\n",
        "        return proc, url\n",
        "\n",
        "    print('Tunnel URL did not become stably reachable in time.')\n",
        "    if proc.poll() is None:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=5)\n",
        "        except Exception:\n",
        "            proc.kill()\n",
        "    return None, url\n",
        "\n",
        "\n",
        "def start_tunnel_with_retries(port=8188, max_attempts=6):\n",
        "    protocols = ['http2', 'quic']\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        proto = protocols[(attempt - 1) % len(protocols)]\n",
        "        print(f\"\\n== Tunnel attempt {attempt}/{max_attempts} ==\")\n",
        "        proc, url = start_cloudflare_tunnel_once(\n",
        "            port=port,\n",
        "            protocol=proto,\n",
        "            read_timeout=150,\n",
        "            warmup_timeout=180,\n",
        "        )\n",
        "        if proc is not None and url:\n",
        "            return proc, url\n",
        "        time.sleep(min(8, 2 + attempt))\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def stop_proc_safely(proc):\n",
        "    if proc is None:\n",
        "        return\n",
        "    if proc.poll() is None:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=5)\n",
        "        except Exception:\n",
        "            proc.kill()\n",
        "\n",
        "\n",
        "def tunnel_thread(port=8188):\n",
        "    if not wait_port('127.0.0.1', port, timeout=240):\n",
        "        print('Timed out waiting for ComfyUI port', port)\n",
        "        return\n",
        "\n",
        "    print('\\nComfyUI port is open. Creating stable tunnel...\\n')\n",
        "\n",
        "    while True:\n",
        "        proc, url = start_tunnel_with_retries(port=port, max_attempts=6)\n",
        "        if proc is None:\n",
        "            print('Failed to establish a reachable Cloudflare tunnel automatically.')\n",
        "            print('Rerun this cell to retry with a fresh tunnel session.')\n",
        "            return\n",
        "\n",
        "        unhealthy_streak = 0\n",
        "        while proc.poll() is None:\n",
        "            time.sleep(15)\n",
        "            if verify_tunnel_url(url, timeout=8):\n",
        "                unhealthy_streak = 0\n",
        "            else:\n",
        "                unhealthy_streak += 1\n",
        "                print(f'Cloudflare tunnel health check failed ({unhealthy_streak}/3).')\n",
        "                if unhealthy_streak >= 3:\n",
        "                    print('Tunnel became unhealthy. Recreating...')\n",
        "                    stop_proc_safely(proc)\n",
        "                    break\n",
        "\n",
        "        if proc.poll() is not None:\n",
        "            rc = proc.returncode\n",
        "            print(f'Cloudflare tunnel exited (code={rc}). Recreating...')\n",
        "\n",
        "\n",
        "threading.Thread(target=tunnel_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "print('Starting ComfyUI... link will appear above.')\n",
        "!python main.py --dont-print-server --port 8188 --lowvram --preview-method auto\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
